{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T12:50:29.923181300Z",
     "start_time": "2024-08-21T12:50:29.914182900Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\incognito\\AppData\\Local\\Temp\\ipykernel_1116\\3541236939.py\", line 5, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\torch\\__init__.py\", line 2120, in <module>\n",
      "    from torch._higher_order_ops import cond\n",
      "  File \"c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\torch\\_higher_order_ops\\__init__.py\", line 1, in <module>\n",
      "    from .cond import cond\n",
      "  File \"c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\torch\\_higher_order_ops\\cond.py\", line 5, in <module>\n",
      "    import torch._subclasses.functional_tensor\n",
      "  File \"c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 42, in <module>\n",
      "    class FunctionalTensor(torch.Tensor):\n",
      "  File \"c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 258, in FunctionalTensor\n",
      "    cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:258: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import tqdm\n",
    "import torch\n",
    "import random\n",
    "import datasets\n",
    "import itertools\n",
    "import transformers\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.optim import Adam\n",
    "from transformers import BertTokenizer\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Module\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f45d5db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 64\n",
    "\n",
    "## Loading all data into memory\n",
    "\n",
    "corpus_movie_conv = '.\\datasets\\movie_conversations.txt'\n",
    "corpus_movie_lines = '.\\datasets\\movie_lines.txt'\n",
    "\n",
    "\n",
    "\n",
    "# Splitting text using special lines\n",
    "with open(corpus_movie_lines, 'r', encoding='iso-8859-1') as l:\n",
    "    lines = l.readlines()\n",
    "\n",
    "lines_dict = {}\n",
    "for line in lines:\n",
    "    objects = line.split(\" +++$+++ \")\n",
    "    lines_dict[objects[0]] = objects[-1]\n",
    "\n",
    "# generating the question answer pair\n",
    "with open (corpus_movie_conv, 'r', encoding='iso-8859-1') as c:\n",
    "    conv = c.readlines()\n",
    "    \n",
    "pairs=[]\n",
    "for con in conv:\n",
    "    ids = eval(con.split(\" +++$+++ \")[-1])\n",
    "    for i in range(len(ids)):\n",
    "        qa_pairs = []\n",
    "        if i == len(ids) -1:\n",
    "            break\n",
    "        first = lines_dict[ids[i]].strip()\n",
    "        second = lines_dict[ids[i+1]].strip()\n",
    "        \n",
    "        qa_pairs.append(''.join(first.strip()[:MAX_LEN]))\n",
    "        qa_pairs.append(''.join(second.strip()[:MAX_LEN]))\n",
    "        pairs.append(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43c4f86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 221616/221616 [00:00<00:00, 3301795.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# WordPiece tokenizer\n",
    "\n",
    "# save data as txt file\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "for sample in tqdm.tqdm([x[0] for x in pairs]):\n",
    "    text_data.append(sample)\n",
    "    \n",
    "    # once we hit 10K mark, save to file\n",
    "    if len(text_data) == 10_000:\n",
    "        with open(f'./data/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "        \n",
    "paths = [str(x) for x in Path('./data').glob('**/*.txt')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "260571d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2165: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# train own tokenizer\n",
    "tokenizer= BertWordPieceTokenizer(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=False,\n",
    "    strip_accents=False,\n",
    "    lowercase=True\n",
    ")\n",
    "    \n",
    "tokenizer.train(\n",
    "    files=paths,\n",
    "    vocab_size=30_000,\n",
    "    min_frequency=5,\n",
    "    limit_alphabet=1000,\n",
    "    wordpieces_prefix='##',\n",
    "    special_tokens=['[PAD]', '[CLS]', '[SEP]', '[MASK]','[UNK]']\n",
    ")    \n",
    "\n",
    "tokenizer.save_model('./bert-it-1','bert-it')\n",
    "tokenizer = BertTokenizer.from_pretrained('./bert-it-1/bert-it-vocab.txt', local_files_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7558e794",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, data_pair, tokenizer, seq_len=64):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.corpus_lines = len(data_pair)\n",
    "        self.lines = data_pair\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.corpus_lines\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        # Step 1: get random sentence pair, either negative or positive (saved as is_next_label)\n",
    "        t1, t2, is_next_label = self.get_sent(item)\n",
    "\n",
    "        # Step 2: replace random words in sentence with mask / random words\n",
    "        t1_random, t1_label = self.random_word(t1)\n",
    "        t2_random, t2_label = self.random_word(t2)\n",
    "\n",
    "        # Step 3: Adding CLS and SEP tokens to the start and end of sentences\n",
    "         # Adding PAD token for labels\n",
    "        t1 = [self.tokenizer.vocab['[CLS]']] + t1_random + [self.tokenizer.vocab['[SEP]']]\n",
    "        t2 = t2_random + [self.tokenizer.vocab['[SEP]']]\n",
    "        t1_label = [self.tokenizer.vocab['[PAD]']] + t1_label + [self.tokenizer.vocab['[PAD]']]\n",
    "        t2_label = t2_label + [self.tokenizer.vocab['[PAD]']]\n",
    "\n",
    "        # Step 4: combine sentence 1 and 2 as one input\n",
    "        # adding PAD tokens to make the sentence same length as seq_len\n",
    "        segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len]  \n",
    "        bert_input = (t1 + t2)[:self.seq_len]\n",
    "        bert_label = (t1_label + t2_label)[:self.seq_len]\n",
    "        padding = [self.tokenizer.vocab['[PAD]'] for _ in range(self.seq_len - len(bert_input))]\n",
    "        bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)\n",
    "\n",
    "        output = {\"bert_input\": bert_input,\n",
    "                  \"bert_label\": bert_label,\n",
    "                  \"segment_label\": segment_label,\n",
    "                  \"is_next\": is_next_label}\n",
    "\n",
    "        return {key: torch.tensor(value) for key, value in output.items()}\n",
    "\n",
    "    def random_word(self, sentence):\n",
    "        tokens = sentence.split()\n",
    "        output_label = []\n",
    "        output = []\n",
    "\n",
    "        # 15% of the tokens would be replaced\n",
    "        for i, token in enumerate(tokens):\n",
    "            prob = random.random()\n",
    "\n",
    "            # remove cls and sep token\n",
    "            token_id = self.tokenizer(token)['input_ids'][1:-1]\n",
    "\n",
    "            if prob < 0.15:\n",
    "                prob /= 0.15\n",
    "\n",
    "                # 80% chance change token to mask token\n",
    "                if prob < 0.8:\n",
    "                    for i in range(len(token_id)):\n",
    "                        output.append(self.tokenizer.vocab['[MASK]'])\n",
    "\n",
    "                # 10% chance change token to random token\n",
    "                elif prob < 0.9:\n",
    "                    for i in range(len(token_id)):\n",
    "                        output.append(random.randrange(len(self.tokenizer.vocab)))\n",
    "\n",
    "                # 10% chance change token to current token\n",
    "                else:\n",
    "                    output.append(token_id)\n",
    "\n",
    "                output_label.append(token_id)\n",
    "\n",
    "            else:\n",
    "                output.append(token_id)\n",
    "                for i in range(len(token_id)):\n",
    "                    output_label.append(0)\n",
    "\n",
    "        # flattening\n",
    "        output = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output]))\n",
    "        output_label = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output_label]))\n",
    "        assert len(output) == len(output_label)\n",
    "        return output, output_label\n",
    "\n",
    "    def get_sent(self, index):\n",
    "        '''return random sentence pair'''\n",
    "        t1, t2 = self.get_corpus_line(index)\n",
    "\n",
    "        # negative or positive pair, for next sentence prediction\n",
    "        if random.random() > 0.5:\n",
    "            return t1, t2, 1\n",
    "        else:\n",
    "            return t1, self.get_random_line(), 0\n",
    "\n",
    "    def get_corpus_line(self, item):\n",
    "        '''return sentence pair'''\n",
    "        return self.lines[item][0], self.lines[item][1]\n",
    "\n",
    "    def get_random_line(self):\n",
    "        '''return random single sentence'''\n",
    "        return self.lines[random.randrange(len(self.lines))][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a98f496a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bert_input': tensor([   1,   48,   11,   52,    3,  178,  537,   16, 3183,    3,    3,    2,\n",
      "         860, 6954,   15,  168,   11,   58,  156,   17,  142,  687,   11,   59,\n",
      "         264, 2305,  264,   48,  256,  142,    3,    3, 7726,   17,    2,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0]), 'bert_label': tensor([    0,    48,    11,    52,  1812,     0,     0,     0,     0, 12208,\n",
      "           17,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "          212,    15,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]), 'segment_label': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'is_next': tensor(0)}\n"
     ]
    }
   ],
   "source": [
    "train_data = BERTDataset(\n",
    "    pairs, seq_len=MAX_LEN, tokenizer=tokenizer\n",
    ")\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, pin_memory=True)\n",
    "\n",
    "sample_data = next(iter(train_loader))\n",
    "print(train_data[random.randrange(len(train_data))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5ca3da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(Module):\n",
    "    def __init__(self, d_model, max_len = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Computer the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.requires_grad = False\n",
    "        \n",
    "        for pos in range(max_len):\n",
    "            # for each dimension of the each position\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10_000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "                \n",
    "        # include the batch size\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.pe          \n",
    "        \n",
    "        \n",
    "class BERTEmbedding(Module):\n",
    "    '''\n",
    "    BERT Embedding which is consisted with under features\n",
    "        1. TokenEmbedding : normal embedding matrix\n",
    "        2. PositionalEmbedding : adding positional information using sin, cos\n",
    "        3. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)\n",
    "        sum of all these features are output of BERTEmbedding\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, seq_len=64, dropout=0.1):\n",
    "        '''\n",
    "        :param vocal_size: total vocab size\n",
    "        :param embed_size: embedding size of token embedding\n",
    "        :param dropout: dropout rate\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.token = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.segment = nn.Embedding(3, embed_size, padding_idx=0)\n",
    "        self.position = PositionalEmbedding(d_model=embed_size, max_len=seq_len)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, sequence, segment_label):\n",
    "        x = self.token(sequence) + self.position(sequence) + self.segment(sequence)\n",
    "        return self.dropout(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24f75efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Attention\n",
    "\n",
    "class MultiHeadedAttention(Module):\n",
    "    def __init__(self, heads, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % heads == 0\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "        \n",
    "        self.query = torch.nn.Linear(d_model, d_model)\n",
    "        self.key = torch.nn.Linear(d_model, d_model)\n",
    "        self.value = torch.nn.Linear(d_model, d_model)\n",
    "        self.output_linear = torch.nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        '''\n",
    "        query, key, value of shape: (batch_size, max_len, d_model\n",
    "        mask of shape: (batch_size, 1, 1, max_words)\n",
    "        '''\n",
    "        # (batch_size, max_len, d_model)\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)        \n",
    "        value = self.value(value) \n",
    "        \n",
    "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
    "        scores = torch.matmul(query, key.permute(0, 1, 3, 2)) / math.sqrt(query.size(-1))\n",
    "        \n",
    "        # fill 0 mask with super small number so it wont affect the softmax weight\n",
    "        # (batch_size, h, max_len, max_len)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)   \n",
    "        \n",
    "        # (batch_size, h, max_len, max_len)\n",
    "        # softmax to put attention weight for all non-pad tokens\n",
    "        # max_len X max_len matrix of attention\n",
    "        weights = F.softmax(scores, dim=-1)           \n",
    "        weights = self.dropout(weights)\n",
    "        \n",
    "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        context = torch.matmul(weights, value)\n",
    "        \n",
    "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, d_model)\n",
    "        context = context.permute(0, 2, 1, 3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
    "\n",
    "         # (batch_size, max_len, d_model)\n",
    "        return self.output_linear(context)\n",
    "        \n",
    "    \n",
    "class FeedForward(Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, middle_dim=2048, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(d_model, middle_dim)\n",
    "        self.fc2 = torch.nn.Linear(middle_dim, d_model)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.activation = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.fc1(x))\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        return out\n",
    "    \n",
    "class EncoderLayer(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model=768,\n",
    "        heads=12,\n",
    "        feed_forward_hidden= 768 * 4,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.self_multihead = MultiHeadedAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model, middle_dim=feed_forward_hidden)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, embeddings, mask):\n",
    "        # embeddings: (batch_size, max_len, d_model)\n",
    "        # encoder mask: (batch_size, 1, 1, max_len)\n",
    "        # result: (batch_size, max_len, d_model)\n",
    "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
    "        # residual layer\n",
    "        interacted = self.layernorm(interacted + embeddings)\n",
    "        # bottleneck\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        encoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return encoded\n",
    "        \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140c20ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
