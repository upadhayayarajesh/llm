{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T12:50:29.923181300Z",
     "start_time": "2024-08-21T12:50:29.914182900Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "import transformers, datasets\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import itertools\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f45d5db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 64\n",
    "\n",
    "## Loading all data into memory\n",
    "\n",
    "corpus_movie_conv = '.\\datasets\\movie_conversations.txt'\n",
    "corpus_movie_lines = '.\\datasets\\movie_lines.txt'\n",
    "\n",
    "\n",
    "\n",
    "# Splitting text using special lines\n",
    "with open(corpus_movie_lines, 'r', encoding='iso-8859-1') as l:\n",
    "    lines = l.readlines()\n",
    "\n",
    "lines_dict = {}\n",
    "for line in lines:\n",
    "    objects = line.split(\" +++$+++ \")\n",
    "    lines_dict[objects[0]] = objects[-1]\n",
    "\n",
    "# generating the question answer pair\n",
    "with open (corpus_movie_conv, 'r', encoding='iso-8859-1') as c:\n",
    "    conv = c.readlines()\n",
    "    \n",
    "pairs=[]\n",
    "for con in conv:\n",
    "    ids = eval(con.split(\" +++$+++ \")[-1])\n",
    "    for i in range(len(ids)):\n",
    "        qa_pairs = []\n",
    "        if i == len(ids) -1:\n",
    "            break\n",
    "        first = lines_dict[ids[i]].strip()\n",
    "        second = lines_dict[ids[i+1]].strip()\n",
    "        \n",
    "        qa_pairs.append(''.join(first.strip()[:MAX_LEN]))\n",
    "        qa_pairs.append(''.join(second.strip()[:MAX_LEN]))\n",
    "        pairs.append(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43c4f86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 221616/221616 [00:00<00:00, 1066740.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# WordPiece tokenizer\n",
    "\n",
    "# save data as txt file\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "for sample in tqdm.tqdm([x[0] for x in pairs]):\n",
    "    text_data.append(sample)\n",
    "    \n",
    "    # once we hit 10K mark, save to file\n",
    "    if len(text_data) == 10_000:\n",
    "        with open(f'./data/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "        \n",
    "paths = [str(x) for x in Path('./data').glob('**/*.txt')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "260571d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2165: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\incognito\\miniconda3\\envs\\llm\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# train own tokenizer\n",
    "tokenizer= BertWordPieceTokenizer(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=False,\n",
    "    strip_accents=False,\n",
    "    lowercase=True\n",
    ")\n",
    "    \n",
    "tokenizer.train(\n",
    "    files=paths,\n",
    "    vocab_size=30_000,\n",
    "    min_frequency=5,\n",
    "    limit_alphabet=1000,\n",
    "    wordpieces_prefix='##',\n",
    "    special_tokens=['[PAD]', '[CLS]', '[SEP]', '[MASK]','[UNK]']\n",
    ")    \n",
    "\n",
    "tokenizer.save_model('./bert-it-1','bert-it')\n",
    "tokenizer = BertTokenizer.from_pretrained('./bert-it-1/bert-it-vocab.txt', local_files_only=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
