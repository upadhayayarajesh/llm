{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gate Value Calculation \n",
    "\n",
    "$\\alpha(E_i) = \\frac{\\sum_{j=0}^{N} \\exp(h \\cdot e_j)}{\\exp(h \\cdot e_i)}$\n",
    "1. **h** is the hidden representation of the input token. **h** belongs to the space $\\mathbb{R}^d$, where **d** is the dimnesionality of hidden state.\n",
    "2. $e_i$ is the trainable embedding for expert $E_i$, which is also in $\\mathbb{R}^d$.The embedding is typically a learned vector for each expert.\n",
    "3. The tern **h.$e_i$** represents the dot product between the hidden representation **h** and the expert embedding $e_i$, resulting in a scalar value.\n",
    "\n",
    "### Output Calculation\n",
    "\n",
    "$ o= h + \\sum_{i=0}^{N} \\ alpha(E_i)\\cdot E_i(h)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden representation h:  (64,)\n",
      "Trainable embeddings e_i: 3 (64,)\n",
      "Dot products h.e_i:  (3,)\n",
      "Gate values (alpha(E_i)):  (3,)\n",
      "Final output after applying LoRA modules and gate values:  (64,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "def softmax(x):\n",
    "    return np.exp(x)/ np.sum(np.exp(x), axis=0)\n",
    "\n",
    "d = 64\n",
    "R = 16\n",
    "num_experts = 3\n",
    "\n",
    "A_matrices = [np.random.rand(d,R) for _  in range(num_experts)]\n",
    "B_matrices = [np.random.rand(R,d) for _ in range(num_experts)]\n",
    "\n",
    "h = np.random.randn(d)\n",
    "\n",
    "trainable_embeddings = [np.random.randn(d) for _ in range(num_experts)]\n",
    "\n",
    "lora_outputs = []\n",
    "\n",
    "for i in range(num_experts):\n",
    "    A_i = A_matrices[i]\n",
    "    B_i = B_matrices[i]\n",
    "    \n",
    "    delta_h_i = B_i.T @ (A_i.T @ h) \n",
    "    lora_outputs.append(delta_h_i)\n",
    "    \n",
    "\n",
    "    \n",
    "dot_products = np.array([np.dot(h, e_i) for e_i in trainable_embeddings])\n",
    "gate_values = softmax(dot_products)\n",
    "\n",
    "final_output = np.zeros_like(h)\n",
    "for i in range(num_experts):\n",
    "    final_output += gate_values[i] * lora_outputs[i]\n",
    "    \n",
    "    \n",
    "print(\"Hidden representation h: \", h.shape)\n",
    "print(\"Trainable embeddings e_i:\",len(trainable_embeddings), trainable_embeddings[0].shape)\n",
    "print(\"Dot products h.e_i: \", dot_products.shape)\n",
    "print(\"Gate values (alpha(E_i)): \", gate_values.shape)\n",
    "print(\"Final output after applying LoRA modules and gate values: \", final_output.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixture of LoRA Experts (MoLE),\n",
    "MoLE extends the transformer architecture by combining outputs from multiple LoRA modules via a gating function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, tau=1.0):\n",
    "    scaled_x = x/ tau\n",
    "    return np.exp(scaled_x)/ np.sum(np.exp(scaled_x), axis=0)\n",
    "\n",
    "d = 64\n",
    "R = 16\n",
    "num_experts = 3\n",
    "L = 10\n",
    "\n",
    "pretrained_params = np.random.randn(d,d)\n",
    "\n",
    "A_matrices = [np.random.randn(d, R) for _ in range(num_experts)]\n",
    "B_matrices = [np.random.randn(R, d) for _ in range(num_experts)]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a1_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
