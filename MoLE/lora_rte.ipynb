{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torchmetrics import Accuracy\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('glue','rte')\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "model_name = 'google/flan-t5-base'\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"premise: {premise} hypothesis: {hypothesis}\" for premise, hypothesis in zip(examples['sentence1'], examples['sentence2'])]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding='max_length')\n",
    "\n",
    "    model_inputs[\"labels\"] = examples[\"label\"]    \n",
    "    return model_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "validation_dataset = validation_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "train_dataset = train_dataset.remove_columns(['sentence1', 'sentence2', 'idx'])\n",
    "validation_dataset = validation_dataset.remove_columns(['sentence1', 'sentence2', 'idx'])\n",
    "\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "validation_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# train_dataset = train_dataset.select(range(1000))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16, \n",
    "    lora_dropout=0.1,  \n",
    "    target_modules=[\"q\", \"v\"]  \n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "classification_head = nn.Linear(model.config.d_model, 3).to(device)\n",
    "optimizer = AdamW(list(model.parameters()) + list(classification_head.parameters()), lr=5e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "epochs = 3 \n",
    "model.train()\n",
    "classification_head.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state[:, 0, :] \n",
    "        logits = classification_head(hidden_state)\n",
    "\n",
    "        loss = loss_fn(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./lora_trained/lora-t5-rte\")  \n",
    "\n",
    "# Load the base model again\n",
    "# base_model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "# peft_model = PeftModel.from_pretrained(base_model, \"lora_model\").to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a1_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
